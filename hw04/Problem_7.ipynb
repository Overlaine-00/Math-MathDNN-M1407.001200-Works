{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7\n",
    "\n",
    "Import necessary libraries and load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our reduced conv2d layer requires different input channel for each output channel. To do this, we build own conv2d layer for each output channel. \\\n",
    "More specifically, suppose an output channel requires $k$ input channels. Then we allocate nn.Conv2d(k, 1, kernel_size=5) to this channel. \\\n",
    "We create 16 such individual conv2d layer, which can be done by nn.ModuleList. The result is following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C3_layer_full(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer_full, self).__init__()\n",
    "        self.conv_layer = nn.Conv2d(6, 16, kernel_size=5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layer(x)\n",
    "\n",
    "\n",
    "class C3_layer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(C3_layer, self).__init__()\n",
    "        self.ch_in_3 = [[0, 1, 2],\n",
    "                        [1, 2, 3],\n",
    "                        [2, 3, 4],\n",
    "                        [3, 4, 5],\n",
    "                        [0, 4, 5],\n",
    "                        [0, 1, 5]] # filter with 3 subset of input channels\n",
    "        self.ch_in_4 = [[0, 1, 2, 3],\n",
    "                        [1, 2, 3, 4],\n",
    "                        [2, 3, 4, 5],\n",
    "                        [0, 3, 4, 5],\n",
    "                        [0, 1, 4, 5],\n",
    "                        [0, 1, 2, 5],\n",
    "                        [0, 1, 3, 4],\n",
    "                        [1, 2, 4, 5],\n",
    "                        [0, 2, 3, 5]] # filter with 4 subset of input channels\n",
    "        # put implementation here\n",
    "        self.layer = nn.ModuleList([nn.Conv2d(3,1,kernel_size=5) for _ in range(6)] + \\\n",
    "                                      [nn.Conv2d(4,1,kernel_size=5) for _ in range(9)] + \\\n",
    "                                      [nn.Conv2d(6,1,kernel_size=5)] )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # put implementation here\n",
    "        out_channel_list = []\n",
    "        for i in range(6):\n",
    "            out_channel_list.append( self.layer[i](x[:, self.ch_in_3[i], :, :]) )\n",
    "        for i in range(9):\n",
    "            out_channel_list.append( self.layer[6+i](x[:, self.ch_in_4[i], :, :]) )\n",
    "        out_channel_list.append( self.layer[15](x) )\n",
    "        \n",
    "        out = torch.cat(out_channel_list, dim=0)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct original LeNet. \\\n",
    "Before then, first we calculate the number of parameters to estimate the 'reduction' effect of parameter, which is the main purpose of original LeNet. \\\n",
    " \\\n",
    "As you can see below, parameters of network appears only at conv2d and linear. \\\n",
    "For each conv2d network nn.Conv2d(a,b,kernel_size=k), the number of parameter is $b*(a*k^2+1)$. Also, each FC layer nn.Linear(a,b) has $a*b+b = (a+1)b$. \\\n",
    "Thus the number of parameter may become $6*26 + (6*76 + 9*101 + 1*126) + 48120 + 10164 + 850 = 60806$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module) :\n",
    "    def __init__(self) :\n",
    "        super(LeNet, self).__init__()\n",
    "        self.C1_layer = nn.Sequential(\n",
    "                nn.Conv2d(1, 6, kernel_size=5, padding=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P2_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C3_layer = nn.Sequential(\n",
    "                #C3_layer_full(),\n",
    "                C3_layer(),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.P4_layer = nn.Sequential(\n",
    "                nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.C5_layer = nn.Sequential(\n",
    "                nn.Linear(5*5*16, 120),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F6_layer = nn.Sequential(\n",
    "                nn.Linear(120, 84),\n",
    "                nn.Tanh()\n",
    "                )\n",
    "        self.F7_layer = nn.Linear(84, 10)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        output = self.C1_layer(x)\n",
    "        output = self.P2_layer(output)\n",
    "        output = self.C3_layer(output)\n",
    "        output = self.P4_layer(output)\n",
    "        output = output.view(-1,5*5*16)\n",
    "        output = self.C5_layer(output)\n",
    "        output = self.F6_layer(output)\n",
    "        output = self.F7_layer(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 60806\n"
     ]
    }
   ],
   "source": [
    "model = LeNet().to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "param_ct = sum([p.numel() for p in model.parameters()])\n",
    "print(f\"Total number of trainable parameters: {param_ct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are exactly 60806 parameters as we expected. \\\n",
    "To verify our network works well, let us train and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch starting.\n",
      "1th epoch starting.\n",
      "2th epoch starting.\n",
      "3th epoch starting.\n",
      "4th epoch starting.\n",
      "5th epoch starting.\n",
      "6th epoch starting.\n",
      "7th epoch starting.\n",
      "8th epoch starting.\n",
      "9th epoch starting.\n",
      "Time ellapsed in training is: 115.97027707099915\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100, shuffle=True)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "for epoch in range(10) :\n",
    "    print(\"{}th epoch starting.\".format(epoch))\n",
    "    for images, labels in train_loader :\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss = loss_function(model(images), labels)\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "end = time.time()\n",
    "\n",
    "print(\"Time ellapsed in training is: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test set] Average loss: 0.0228, Accuracy: 1619/10000 (16.19%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss, correct, total = 0, 0, 0\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=100, shuffle=False)\n",
    "\n",
    "for images, labels in test_loader :\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "    output = model(images)\n",
    "    test_loss += loss_function(output, labels).item()\n",
    "\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    correct += pred.eq(labels.view_as(pred)).sum().item()\n",
    "    \n",
    "    total += labels.size(0)\n",
    "            \n",
    "print('[Test set] Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss /total, correct, total,\n",
    "        100. * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is quite poor, because we didn't go through enoughly many training epoch. \\\n",
    "However our object is accomplished, because our model shows larger accuracy than random choice(10% accuracy). \\\n",
    "In fact, when I repeat training process (i.e., increase epoch), accuracy gets higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b317da953ad838286c0c6ab0dfc4969c0da5f5eb296aba9f813b609fe07b4eeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
