{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "Import necessary libraries and load dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn, tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load dataset, and filter whose label is 4 or 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1, label_2 = 4, 9\n",
    "\n",
    "train_set = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "idx = (train_set.targets == label_1) + (train_set.targets == label_2)\n",
    "train_set.data = train_set.data[idx]\n",
    "train_set.targets = train_set.targets[idx]\n",
    "train_set.targets[train_set.targets == label_1] = -1\n",
    "train_set.targets[train_set.targets == label_2] = 1\n",
    "\n",
    "test_set = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "idx = (test_set.targets == label_1) + (test_set.targets == label_2)\n",
    "test_set.data = test_set.data[idx]\n",
    "test_set.targets = test_set.targets[idx]\n",
    "test_set.targets[test_set.targets == label_1] = -1\n",
    "test_set.targets[test_set.targets == label_2] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we compare logistic regerssion and L2-loss. \\\n",
    "Before starting, let us define a linear term containing in $f$. This will be used in both case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module) :\n",
    "    def __init__(self, input_dim=28*28) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, x) :\n",
    "        return self.linear(x.float().view(-1, 28*28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we perform with logistic regerssion. \\\n",
    "We use log-sigmoid as loss, and SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LR()                                   # Define a Neural Network Model\n",
    "\n",
    "def logistic_loss(output, target):\n",
    "    return -torch.nn.functional.logsigmoid(target*output)\n",
    "\n",
    "loss_function = logistic_loss                                                   # Specify loss function\n",
    "optimizer = torch.optim.SGD(logistic.parameters(), lr=1e-4)   # specify SGD with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "for _ in range(1000) :\n",
    "    ind = random.randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss = loss_function(logistic(image), label.float())\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression summary\n",
      "         Time ellapsed in training is: 0.24887609481811523, Accuracy: 1884/1991\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_set.data)):\n",
    "        predicted = logistic(test_set.data[i])\n",
    "        \n",
    "        if predicted.item() * test_set.targets[i].item() >= 0:\n",
    "            correct += 1\n",
    "print(f\"Logistic Regression summary\\n \\\n",
    "        Time ellapsed in training is: {end-start}, Accuracy: {correct}/{len(test_set.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have quite high (almost 92%) accuracy in test step. \\\n",
    "Next, perform with L2-loss, introduced in hint. Everything except loss function is same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2-loss Regression summary\n",
      " Time ellapsed in training is: 0.3280670642852783, Accuracy: 1242/1991\n"
     ]
    }
   ],
   "source": [
    "l2 = LR()\n",
    "\n",
    "def l2_loss_function(output, target):\n",
    "    return torch.sigmoid(-target*output)**2 + (1-torch.sigmoid(target*output))**2\n",
    "\n",
    "loss_function = l2_loss_function\n",
    "optimizer = torch.optim.SGD(l2.parameters(), lr=1e-4)\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(1000) :\n",
    "    ind = random.randint(0, len(train_set.data)-1)\n",
    "    image, label = train_set.data[ind], train_set.targets[ind]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_loss = loss_function(l2(image), label.float())\n",
    "    train_loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "end = time.time()\n",
    "\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_set.data)):\n",
    "        predicted = l2(test_set.data[i])\n",
    "        if predicted[0].item() * test_set.targets[i].item() >= 0:\n",
    "            correct += 1\n",
    "print(f\"L2-loss Regression summary\\n \\\n",
    "Time ellapsed in training is: {end-start}, Accuracy: {correct}/{len(test_set.data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike logistic, L2 shows poor performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b317da953ad838286c0c6ab0dfc4969c0da5f5eb296aba9f813b609fe07b4eeb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
